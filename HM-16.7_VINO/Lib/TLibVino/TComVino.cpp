#include "TComVino.h"

#include <fstream>
#include <iomanip>
#include <vector>
#include <memory>
#include <string>
#include <cstdlib>

//#include <opencv2/opencv.hpp>
#include <inference_engine.hpp>

using namespace InferenceEngine;

//#define max(x,y) (x>y)?x:y
//#define min(x,y) (x>y)?y:x
//#define  Clip3(l,h,x) max(min(h,x),l)

void pic_cpy(float_t* dst, TComPicYuv* rec_pic, Rect patch, int stride_dst, ComponentID CompID = COMPONENT_Y)
{
	Pel* rec_pic_y = rec_pic->getAddr(CompID);
	int height = rec_pic->getHeight(CompID);
	int width = rec_pic->getWidth(CompID);
	int stride = rec_pic->getStride(CompID);
	// int stride_dst = patch.endX - patch.startX + 26;
	dst += 21 - patch.startX + (21 - patch.startY) * stride_dst;

	int i, j;
	for (i = ((patch.startY == 0) ? 0 : (patch.startY - 21)); i < ((patch.endY == height) ? height : (patch.endY + 21)); i++)
	{
		for (j = ((patch.startX == 0) ? 0 : (patch.startX - 21)); j < ((patch.endX == width) ? width : (patch.endX + 21)); j++)
			dst[i*stride_dst + j] = float(rec_pic_y[i*stride + j]) / 256.;

		if (patch.startX == 0)
			for (j = -21; j < 0; j++)
				dst[i*stride_dst + j] = float(rec_pic_y[i*stride + (-j - 1)]) / 256.;

		if (patch.endX == width)
			for (j = width; j < width + 21; j++)
				dst[i*stride_dst + j] = float(rec_pic_y[i*stride + (2 * width - j - 1)]) / 256.;
	}

	if (patch.startY == 0)
		for (i = -21; i < 0; i++)
		{
			for (j = patch.startX - 21; j < patch.endX + 21; j++)
				dst[i*stride_dst + j] = dst[(-i - 1)*stride_dst + j];
		}

	if (patch.endY == height)
		for (int i = height; i < height + 21; i++)
		{
			for (j = patch.startX - 21; j < patch.endX + 21; j++)
				dst[i*stride_dst + j] = dst[(2 * height - i - 1)*stride_dst + j];
		}
}

void pic_cpy(TComPic* pcPic, TComPicYuv* rec_pic, float_t* src, Rect patch, int stride_dst, ComponentID CompID = COMPONENT_Y)
{
	//TComPicYuv* rec_pic = pcPic->getPicYuvRec();
	Pel* rec_pic_y = rec_pic->getAddr(CompID);
	//int height = rec_pic->getHeight(CompID);
	int width = rec_pic->getWidth(CompID);
	int strideY = rec_pic->getStride(CompID);
	//int stride_dst = patch.endX - patch.startX + 16;
	src += 21 - patch.startX + (21 - patch.startY) * stride_dst;//16->21

	TComPicYuv* pred_pic = pcPic->p_pcPicYuvPred;
	Pel* pred_pic_y = pred_pic->getAddr(CompID);
	int strideP = pred_pic->getStride(CompID);

	int pLineRec, pLineDst;
	if (pcPic->getSlice(0)->getSliceType() == I_SLICE)
	{
		for (int i = patch.startY; i < patch.endY; i++)
			for (int j = patch.startX; j < patch.endX; j++)
				rec_pic_y[i*strideY + j] = Clip3((Pel)0, (Pel)255, (Pel)(src[i*stride_dst + j] * 256.));//
	}
	else
	{
		int ctu_idx, row, column;
		for (int i = patch.startY; i < patch.endY; i++)
		{
			pLineRec = i * strideY;
			pLineDst = i * stride_dst;
			for (int j = patch.startX; j < patch.endX; j++)
			{
			    if (pred_pic_y[i*strideP + j] != rec_pic_y[pLineRec + j])
				rec_pic_y[pLineRec + j] = Clip3((Pel)0, (Pel)255, (Pel)(src[pLineDst + j] * 256.));//
			}
		}
	}
}


//#define FPGA 1
void run_model(TComPic* pcPic, TComPicYuv* dstPic)
{

/*
#ifdef FPGA
	const std::string input_model = "./FP16/DLF_320x256_fp16.xml" ;
	const std::string device_name = "HETERO:FPGA,CPU" ;
#else
        const std::string input_model = "./FP32/DLF_320x256_fp32.xml" ;
	const std::string device_name = "CPU" ;
#endif
*/
	TComPicYuv* rec_pic = pcPic->getPicYuvRec();

	ifstream ifs;
	ifs.open("/opt/intel/2019_r1/openvino/deployment_tools/inference_engine/samples/my_build1/intel64/workspace/workspace/workfile.cfg", ios::in);

	if(!ifs.is_open())
	{
	  std::cout<<"模型参数传入失败！"<<std::endl;
	  return;	
	}

	string input_model;
	string device_name;

	getline(ifs,input_model);
	getline(ifs,device_name);
 	
	ifs.close();

    static InferencePlugin* p_plugin = NULL;
	static CNNNetReader* p_network_reader =	NULL;
	static CNNNetwork* p_network = NULL;
	static InputsDataMap* p_input_info = NULL;
	static OutputsDataMap* p_output_info = NULL;
	static ExecutableNetwork* p_executable_network = NULL;
	static InferRequest* p_async_infer_request = NULL;
	
        
     if (p_plugin==NULL)
     {
		std::cout<<"IR路径： "<<input_model<<std::endl;
		std::cout<<"设备名称： "<<device_name<<std::endl;
		// --------------------------- 1. Load Plugin for inference engine -------------------------------------
		p_plugin = new InferencePlugin(PluginDispatcher().getPluginByDevice(device_name));
		InferencePlugin& plugin= *p_plugin;
		// -----------------------------------------------------------------------------------------------------

		// --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
		p_network_reader = new CNNNetReader;
		CNNNetReader& network_reader = *p_network_reader;
		network_reader.ReadNetwork(input_model);
		network_reader.ReadWeights(input_model.substr(0, input_model.size() - 4) + ".bin");
		network_reader.getNetwork().setBatchSize(2);   
		p_network = new CNNNetwork(network_reader.getNetwork());
		CNNNetwork& network = *p_network; 



		// --------------------------- 3. Configure input & output ---------------------------------------------

		// --------------------------- Prepare input blobs -----------------------------------------------------
		/** Taking information about all topology inputs **/
		p_input_info = new InputsDataMap(network.getInputsInfo());
		// ------------------------------ Prepare output blobs -------------------------------------------------
		/** Taking information about all topology outputs **/
		 p_output_info = new OutputsDataMap(network.getOutputsInfo());

		// --------------------------- 4. Loading model to the plugin ------------------------------------------
		 p_executable_network = new ExecutableNetwork(plugin.LoadNetwork(network, {}));

		// --------------------------- 5. Create infer request -------------------------------------------------
		 p_async_infer_request = new InferRequest(p_executable_network->CreateInferRequest());
	}
    
		InferencePlugin& plugin= *p_plugin;
		CNNNetReader& network_reader= *p_network_reader;
		CNNNetwork& network = *p_network;
		InputsDataMap& input_info = *p_input_info;
		OutputsDataMap& output_info = *p_output_info;
		ExecutableNetwork& executable_network = *p_executable_network;
		InferRequest& async_infer_request = *p_async_infer_request;


		int picHeight = rec_pic->getHeight(COMPONENT_Y);
		int picWidth = rec_pic->getWidth(COMPONENT_Y);

		Rect patch = { 0,0,0,0 };
		Rect patch_last = { 0,0,0,0 };
		//int width, height;

		const int PSW = 128;
		const int PSH = 96;
	
		bool odd = false;
		for (patch.startX = 0; patch.startX < picWidth; patch.startX += PSW)
		{
			if (patch.startX + PSW < picWidth)
				patch.endX = patch.startX + PSW;
			else
			{
				patch.endX = picWidth;
				patch.startX = picWidth - PSW; //
				picWidth = 0;
			}
			//width = patch.endX - patch.startX + 42;
			picHeight = rec_pic->getHeight(COMPONENT_Y);
			for (patch.startY = 0; patch.startY < picHeight; patch.startY += PSH)
			{
				if (patch.startY + PSH < picHeight)//- 32)
					patch.endY = patch.startY + PSH;
				else
				{
					patch.endY = picHeight;
					patch.startY = picHeight - PSH; //
					picHeight = 0;
				}
				//height = patch.endY - patch.startY + 42;

				// --------------------------- 6. Process input -------------------------------------------------------
		        if (odd == false && (picHeight>0 || picWidth > 0))
		        {
				  patch_last = patch;
		          odd = true;
				  continue;
		        }
		        else
		        {
		           odd = false;
		        }
		        int input_idx=0;
				for (auto &item : input_info) 
				{
					auto input_name = item.first;
					//InputInfo::Ptr input_data = item.second;
					//std::cout<<input_name<<std::endl;

					/** Getting input blob **/
					Blob::Ptr input = async_infer_request.GetBlob(input_name);
					auto input_buffer = input->buffer().as<PrecisionTrait<Precision::FP32>::value_type *>();

					auto dims = input->getTensorDesc().getDims();

					////////////////////////////////////////////////////////////////////////////////////////////////////
		                        
		            if (input_idx == 0)
					{
						pic_cpy(input_buffer, rec_pic, patch_last, dims[3]);
		                pic_cpy(input_buffer + dims[1]*dims[2]*dims[3], rec_pic, patch, dims[3]);
		            }    
		            else
					{
						pic_cpy(input_buffer, pcPic->p_pcPicYuvPred, patch_last, dims[3]);
		                pic_cpy(input_buffer + dims[1]*dims[2]*dims[3], pcPic->p_pcPicYuvPred, patch, dims[3]);
		            }
					input_idx++;
					//std::cout <<"输入维度"<<input_idx<<": "<< dims[0] << " "<<dims[1] <<" "<< dims[2] <<" "<< dims[3] << std::endl;                           
		            //std::cout<<input_idx<<std::endl;    
				}
		        
				// --------------------------- 7. Inference -------------------------------------------------------
			
				std::cout<<"infer start...\n";
				async_infer_request.Infer();      	  //同步推断
				//async_infer_request.StartAsync();   //异步推断
				//async_infer_request.Wait(IInferRequest::WaitMode::RESULT_READY);
		        std::cout<<"infer finished.\n";
		        
				// --------------------------- 8. Process output -------------------------------------------------------
				for (auto &item : output_info) 
				{
					auto output_name = item.first;

					Blob::Ptr output = async_infer_request.GetBlob(output_name);

					auto output_buffer = output->buffer().as<PrecisionTrait<Precision::FP32>::value_type *>();

					auto dims = output->getTensorDesc().getDims();
					//std::cout <<"输出维度： "<< dims[0] << " "<<dims[1] <<" "<< dims[2] <<" "<< dims[3] << std::endl;
		            pic_cpy(pcPic, dstPic, output_buffer + dims[1]*dims[2]*dims[3], patch, dims[3]);
					pic_cpy(pcPic, dstPic, output_buffer, patch_last, dims[3]);
				}
			}
		}

}

